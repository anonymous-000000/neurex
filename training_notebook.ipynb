{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uiW-hyaBNTiu"
      ],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Packages Downloading"
      ],
      "metadata": {
        "id": "VzEj5_KWVoV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m pip install seqeval==1.2.2 bertviz==1.2.0 umap-learn==0.5.1\n",
        "!python -m pip install transformers==4.27.4 datasets==2.8.0\n",
        "!python -m pip install matplotlib ipywidgets\n",
        "!pip install scikit-multilearn\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "7N0ujWwYk71-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wandb"
      ],
      "metadata": {
        "id": "DoThAwv4DgfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "fkWeKZ6aDip2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "ZHRXeKjxDjoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up project name\n",
        "%env WANDB_PROJECT=EH(pos&neg)\n",
        "\n",
        "# If load_best_model_at_end=True is passed to Trainer, then W&B will save the best performing model to Artifacts.\n",
        "# https://docs.wandb.ai/guides/integrations/huggingface#turn-on-model-versioning\n",
        "%env WANDB_LOG_MODEL='end'"
      ],
      "metadata": {
        "id": "SfcdiaZcDmGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wandb end"
      ],
      "metadata": {
        "id": "hIf5nZSaDrsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ],
      "metadata": {
        "id": "EwRIP6BNFPQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4569ae44-a16c-4212-94f3-1ce51d677991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.0.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb"
      ],
      "metadata": {
        "id": "DngpzVMplImr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html"
      ],
      "metadata": {
        "id": "7eC6kE-EVyCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import datasets\n",
        "import huggingface_hub\n",
        "import transformers\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ],
      "metadata": {
        "id": "wcX52rzul12F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth',1000)"
      ],
      "metadata": {
        "id": "GZUAmFk0rYz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  # Get the GPU device name.\n",
        "  device_name = torch.cuda.get_device_name()\n",
        "  n_gpu = torch.cuda.device_count()\n",
        "  print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")"
      ],
      "metadata": {
        "id": "HMijF0StEJ_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed\n",
        "from transformers import set_seed\n",
        "set_seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "-Y0uAyvmhhpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrnadEUjp9vA",
        "outputId": "3abe25c0-05fd-4fc9-e3eb-6fadd9977d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = ['ClassCastException',\n",
        " 'SecurityException',\n",
        " 'UnsupportedOperationException',\n",
        " 'NoSuchAlgorithmException',\n",
        " 'SQLException',\n",
        " 'IOException',\n",
        " 'NoSuchMethodException',\n",
        " 'IllegalArgumentException',\n",
        " 'NullPointerException',\n",
        " 'FileNotFoundException',\n",
        " 'MalformedURLException',\n",
        " 'InterruptedException',\n",
        " 'JSONException',\n",
        " 'UnsupportedEncodingException',\n",
        " 'com.google.protobuf.InvalidProtocolBufferException',\n",
        " 'IllegalStateException',\n",
        " 'IllegalAccessException',\n",
        " 'URISyntaxException',\n",
        " 'ExecutionException',\n",
        " 'InvalidArgumentException',\n",
        " 'SAXException',\n",
        " 'NumberFormatException',\n",
        " 'ClassNotFoundException',\n",
        " 'RuntimeException',\n",
        " 'GenericEntityException',\n",
        " 'InvocationTargetException',\n",
        " 'ParseException',\n",
        " 'IndexOutOfBoundsException',\n",
        " 'InstantiationException',\n",
        " 'com.google.protobuf.UninitializedMessageException']"
      ],
      "metadata": {
        "id": "MsGHH116OGdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfp93QOJOK0n",
        "outputId": "299b9080-b339-428b-dc8f-ff948f725897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit([all_labels])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "9oX04hoiOMMq",
        "outputId": "c8e718b9-862c-4ade-dc39-31f659815a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelBinarizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Dataset"
      ],
      "metadata": {
        "id": "asvKnUwBAz8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
        "from datasets import DatasetDict, Dataset\n",
        "from datasets import ClassLabel, Sequence, Value\n",
        "from random import randint"
      ],
      "metadata": {
        "id": "-r4o8g7zG34m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.load import load_from_disk\n",
        "# startIndex starts with 0, and the endIndex is inclusive\n",
        "# Duplicates have been removed from \"train\" and \"valid\" splits, but not from the \"test\" split\n",
        "encoded_ds = load_from_disk(\"PATH-TO-THE-TRAINING-DATASET-FOLDER\")"
      ],
      "metadata": {
        "id": "y-E0uou9KwxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xOMIYUbn8a0",
        "outputId": "afcbe1b4-15c1-4445-e107-1f77cc760d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['attention_mask', 'excep_count', 'excep_ids', 'excep_index', 'id', 'input_ids', 'label_cls', 'labels'],\n",
              "        num_rows: 246118\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['attention_mask', 'excep_count', 'excep_ids', 'excep_index', 'id', 'input_ids', 'label_cls', 'labels'],\n",
              "        num_rows: 30764\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['attention_mask', 'excep_count', 'excep_ids', 'excep_index', 'id', 'input_ids', 'label_cls', 'labels'],\n",
              "        num_rows: 30764\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "6dCwkg8JV-Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "codebert_model_name = \"microsoft/codebert-base-mlm\"\n",
        "codebert_tokenizer = AutoTokenizer.from_pretrained(codebert_model_name)"
      ],
      "metadata": {
        "id": "3DAl1EaXNiaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {0:\"O\", 1:\"B-Try\", 2:\"I-Try\"}\n",
        "tag2index = {v:k for k, v in index2tag.items()}"
      ],
      "metadata": {
        "id": "vl5A4g9yNA_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "n = len(index2tag)\n",
        "print(\"num_labels = \", n)\n",
        "\n",
        "codebert_config = AutoConfig.from_pretrained(codebert_model_name,\n",
        "                                             num_labels=n,\n",
        "                                             id2label=index2tag, label2id=tag2index,\n",
        "                                             output_hidden_states=False\n",
        "                                            )\n",
        "\n",
        "codebert_config.num_cls_labels=2\n",
        "codebert_config.num_excep_labels=len(all_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBSGCW2bNCiL",
        "outputId": "5d641eca-f2b9-4f9e-c6c8-99825a2fa010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_labels =  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import RobertaConfig\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
        "from torch_scatter import scatter\n",
        "\n",
        "class CodebertForExcepPrediction(RobertaPreTrainedModel):\n",
        "    config_class = RobertaConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.num_cls_labels = config.num_cls_labels\n",
        "        # Load model body\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "\n",
        "        # Set up heads for three tasks\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.token_classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.cls_classifier = nn.Linear(config.hidden_size, config.num_cls_labels)\n",
        "        self.excep_classifier = nn.Linear(config.hidden_size, config.num_excep_labels)\n",
        "\n",
        "        # Load and initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "                labels=None, label_cls=None,\n",
        "                excep_ids=None, excep_count=None, excep_index=None, **kwargs):\n",
        "\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids, **kwargs)\n",
        "\n",
        "        # Apply classifier to encoder representation\n",
        "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
        "        token_logits = self.token_classifier(sequence_output)\n",
        "        cls_logits = self.cls_classifier(sequence_output[:, 0])\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss_fct2 = nn.MultiLabelSoftMarginLoss(reduction=\"none\")\n",
        "        # Calculate sentence classification losses\n",
        "        cls_loss = None\n",
        "        if label_cls is not None:\n",
        "          cls_loss = loss_fct(cls_logits, label_cls)\n",
        "\n",
        "        # Calculate stmts classification losses\n",
        "        stmt_loss = None\n",
        "        excep_loss = 0.0\n",
        "        mask = label_cls.clone().detach().bool().requires_grad_(False)\n",
        "        if (labels is not None) and torch.any(mask):\n",
        "            # Calculate stmts classification loss\n",
        "            token_logits_masked = token_logits[mask]\n",
        "            labels_masked = labels[mask]\n",
        "            stmt_loss = loss_fct(token_logits_masked.view(-1, self.num_labels), labels_masked.view(-1))\n",
        "\n",
        "            # Calculate the exception prediction loss\n",
        "            sequence_output_masked = sequence_output[mask]\n",
        "            excep_ids_masked = excep_ids[mask]\n",
        "            excep_count_masked = excep_count[mask]\n",
        "            excep_index_masked = excep_index[mask]\n",
        "\n",
        "            for out, ids, count, index in zip(sequence_output_masked, excep_ids_masked, excep_count_masked, excep_index_masked):\n",
        "              h = scatter(out, index, dim=0, reduce=\"sum\")[:count, :]\n",
        "              excep_loss += torch.sum(loss_fct2(self.excep_classifier(h), ids[:count, :]))\n",
        "\n",
        "        loss = None\n",
        "        if stmt_loss is None:\n",
        "          loss = cls_loss\n",
        "        else:\n",
        "          loss = cls_loss + stmt_loss + excep_loss / mask.sum()\n",
        "        # Return model output object\n",
        "        return {\"loss\": loss, \"logits\":token_logits, \"cls_logits\": cls_logits,\n",
        "                \"last_hidden_state\": outputs.last_hidden_state, \"attentions\": outputs.attentions,\n",
        "                \"label_cls\": label_cls, \"labels\": labels}"
      ],
      "metadata": {
        "id": "FMpVP6ThuDYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "import evaluate\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    seqeval = evaluate.load('seqeval')\n",
        "    logits = eval_pred.predictions[0]\n",
        "    labels = eval_pred.label_ids[0]\n",
        "    cls_logits = eval_pred.predictions[1]\n",
        "    cls_true = eval_pred.label_ids[1]\n",
        "\n",
        "    y_pred, y_true = align_predictions(logits, labels)\n",
        "    cls_pred = np.argmax(cls_logits, axis=-1).flatten()\n",
        "\n",
        "    results = seqeval.compute(predictions=y_pred, references=y_true, mode=\"strict\", scheme=\"IOB2\")\n",
        "\n",
        "    return {\"cls_acc\": metrics.accuracy_score(cls_true, cls_pred), \"cls_precision\": metrics.precision_score(cls_true, cls_pred),\n",
        "            \"cls_recall\": metrics.recall_score(cls_true, cls_pred), \"cls_f1\": metrics.f1_score(cls_true, cls_pred),\n",
        "            \"overall_accuracy\": results[\"overall_accuracy\"], \"precision\": results[\"Try\"][\"precision\"],\n",
        "            \"recall\": results[\"Try\"][\"recall\"], \"f1\": results[\"Try\"][\"f1\"]}"
      ],
      "metadata": {
        "id": "A2V1fnvrM4H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_predictions(logits, label_ids):\n",
        "  preds = np.argmax(logits, axis=2)\n",
        "  batch_size, seq_len = preds.shape # (bs, 512)\n",
        "  labels_list, preds_list = [], []\n",
        "\n",
        "  for batch_idx in range(batch_size):\n",
        "    example_labels, example_preds = [], []\n",
        "\n",
        "    for seq_idx in range(seq_len):\n",
        "      # Ignore label IDs = -100\n",
        "      if label_ids[batch_idx, seq_idx] != -100:\n",
        "        example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
        "        example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
        "\n",
        "    labels_list.append(example_labels)\n",
        "    preds_list.append(example_preds)\n",
        "\n",
        "  return preds_list, labels_list"
      ],
      "metadata": {
        "id": "78i-F6qVNO0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def model_init():\n",
        "    return (CodebertForExcepPrediction\n",
        "              .from_pretrained(codebert_model_name, config=codebert_config)\n",
        "              .to(device))"
      ],
      "metadata": {
        "id": "6H56_2IWNOxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(codebert_tokenizer)"
      ],
      "metadata": {
        "id": "jjJYc1DGNOvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Trainer"
      ],
      "metadata": {
        "id": "uiW-hyaBNTiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Mapping\n",
        "from transformers import Trainer\n",
        "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union"
      ],
      "metadata": {
        "id": "jU1X9HW2NOlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def atleast_1d(tensor_or_array: Union[torch.Tensor, np.ndarray]):\n",
        "    if isinstance(tensor_or_array, torch.Tensor):\n",
        "        if hasattr(torch, \"atleast_1d\"):\n",
        "            tensor_or_array = torch.atleast_1d(tensor_or_array)\n",
        "        elif tensor_or_array.ndim < 1:\n",
        "            tensor_or_array = tensor_or_array[None]\n",
        "    else:\n",
        "        tensor_or_array = np.atleast_1d(tensor_or_array)\n",
        "    return tensor_or_array"
      ],
      "metadata": {
        "id": "K6HgdxR_NvnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def torch_pad_and_concatenate(tensor1, tensor2, padding_index=-100):\n",
        "    \"\"\"Concatenates `tensor1` and `tensor2` on first axis, applying padding on the second if necessary.\"\"\"\n",
        "    tensor1 = atleast_1d(tensor1)\n",
        "    tensor2 = atleast_1d(tensor2)\n",
        "\n",
        "    if len(tensor1.shape) == 1 or tensor1.shape[1] == tensor2.shape[1]:\n",
        "        return torch.cat((tensor1, tensor2), dim=0)\n",
        "\n",
        "    # Let's figure out the new shape\n",
        "    new_shape = (tensor1.shape[0] + tensor2.shape[0], max(tensor1.shape[1], tensor2.shape[1])) + tensor1.shape[2:]\n",
        "\n",
        "    # Now let's fill the result tensor\n",
        "    result = tensor1.new_full(new_shape, padding_index)\n",
        "    result[: tensor1.shape[0], : tensor1.shape[1]] = tensor1\n",
        "    result[tensor1.shape[0] :, : tensor2.shape[1]] = tensor2\n",
        "    return result"
      ],
      "metadata": {
        "id": "hRWIwxhWNvkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numpy_pad_and_concatenate(array1, array2, padding_index=-100):\n",
        "    \"\"\"Concatenates `array1` and `array2` on first axis, applying padding on the second if necessary.\"\"\"\n",
        "    array1 = atleast_1d(array1)\n",
        "    array2 = atleast_1d(array2)\n",
        "\n",
        "    if len(array1.shape) == 1 or array1.shape[1] == array2.shape[1]:\n",
        "        return np.concatenate((array1, array2), axis=0)\n",
        "\n",
        "    # Let's figure out the new shape\n",
        "    new_shape = (array1.shape[0] + array2.shape[0], max(array1.shape[1], array2.shape[1])) + array1.shape[2:]\n",
        "\n",
        "    # Now let's fill the result tensor\n",
        "    result = np.full_like(array1, padding_index, shape=new_shape)\n",
        "    result[: array1.shape[0], : array1.shape[1]] = array1\n",
        "    result[array1.shape[0] :, : array2.shape[1]] = array2\n",
        "    return result"
      ],
      "metadata": {
        "id": "0u8pfIUYNviZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_concat(tensors, new_tensors, padding_index=-100):\n",
        "    \"\"\"\n",
        "    Concat the `new_tensors` to `tensors` on the first dim and pad them on the second if needed. Works for tensors or\n",
        "    nested list/tuples/dict of tensors.\n",
        "    \"\"\"\n",
        "    assert type(tensors) == type(\n",
        "        new_tensors\n",
        "    ), f\"Expected `tensors` and `new_tensors` to have the same type but found {type(tensors)} and {type(new_tensors)}.\"\n",
        "    if isinstance(tensors, (list, tuple)):\n",
        "        return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n",
        "    elif isinstance(tensors, torch.Tensor):\n",
        "        return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n",
        "    elif isinstance(tensors, Mapping):\n",
        "        return type(tensors)(\n",
        "            {k: nested_concat(t, new_tensors[k], padding_index=padding_index) for k, t in tensors.items()}\n",
        "        )\n",
        "    elif isinstance(tensors, np.ndarray):\n",
        "        return numpy_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n",
        "    else:\n",
        "        raise TypeError(f\"Unsupported type for concatenation: got {type(tensors)}\")"
      ],
      "metadata": {
        "id": "5yo0BilYNvgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_detach(tensors):\n",
        "    \"Detach `tensors` (even if it's a nested list/tuple/dict of tensors).\"\n",
        "    if isinstance(tensors, (list, tuple)):\n",
        "        return type(tensors)(nested_detach(t) for t in tensors)\n",
        "    elif isinstance(tensors, Mapping):\n",
        "        return type(tensors)({k: nested_detach(t) for k, t in tensors.items()})\n",
        "    return tensors.detach()"
      ],
      "metadata": {
        "id": "jjs5Idk8N1F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "  def prediction_step(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "        prediction_loss_only: bool,\n",
        "        ignore_keys: Optional[List[str]] = ['last_hidden_state', 'attentions', 'label_cls', 'labels'],\n",
        "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Perform an evaluation step on `model` using `inputs`.\n",
        "\n",
        "        Subclass and override to inject custom behavior.\n",
        "\n",
        "        Args:\n",
        "            model (`nn.Module`):\n",
        "                The model to evaluate.\n",
        "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
        "                The inputs and targets of the model.\n",
        "\n",
        "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
        "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
        "            prediction_loss_only (`bool`):\n",
        "                Whether or not to return the loss only.\n",
        "            ignore_keys (`Lst[str]`, *optional*):\n",
        "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
        "                gathering predictions.\n",
        "\n",
        "        Return:\n",
        "            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
        "            logits and labels (each being optional).\n",
        "        \"\"\"\n",
        "        has_labels = False if len(self.label_names) == 0 else all(inputs.get(k) is not None for k in self.label_names)\n",
        "        # For CLIP-like models capable of returning loss values.\n",
        "        # If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`\n",
        "        # is `True` in `model.forward`.\n",
        "        return_loss = inputs.get(\"return_loss\", None)\n",
        "        if return_loss is None:\n",
        "            return_loss = self.can_return_loss\n",
        "        loss_without_labels = True if len(self.label_names) == 0 and return_loss else False\n",
        "\n",
        "\n",
        "        ignore_keys = ['last_hidden_state', 'attentions', 'label_cls', 'labels']\n",
        "\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
        "        if has_labels or loss_without_labels:\n",
        "            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
        "            if len(labels) == 1:\n",
        "                labels = labels[0]\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if has_labels or loss_without_labels:\n",
        "                with self.compute_loss_context_manager():\n",
        "                    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
        "                loss = loss.mean().detach()\n",
        "\n",
        "                if isinstance(outputs, dict):\n",
        "                    logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n",
        "\n",
        "                else:\n",
        "                    logits = outputs[1:]\n",
        "            else:\n",
        "                loss = None\n",
        "                with self.compute_loss_context_manager():\n",
        "                    outputs = model(**inputs)\n",
        "                if isinstance(outputs, dict):\n",
        "                    logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n",
        "                else:\n",
        "                    logits = outputs\n",
        "\n",
        "                if self.args.past_index >= 0:\n",
        "                    self._past = outputs[self.args.past_index - 1]\n",
        "\n",
        "        if prediction_loss_only:\n",
        "            return (loss, None, None)\n",
        "\n",
        "        logits = nested_detach(logits)\n",
        "        if len(logits) == 1:\n",
        "            logits = logits[0]\n",
        "\n",
        "        return (loss, logits, labels)"
      ],
      "metadata": {
        "id": "mrOmjebeN1Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End: Custom Trainer"
      ],
      "metadata": {
        "id": "8XXgYhd_N5N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "num_epochs = 15\n",
        "batch_size = 32\n",
        "logging_steps = len(encoded_ds[\"train\"]) // batch_size\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"PATH-TO-FOLDER\", # save model ckpts\n",
        "    log_level=\"error\",\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=6e-6,\n",
        "    weight_decay=0.01,\n",
        "    metric_for_best_model=\"loss\"\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=logging_steps,\n",
        "    save_strategy = \"epoch\",\n",
        "    gradient_accumulation_steps=1,\n",
        "    disable_tqdm=False,\n",
        "    report_to=\"wandb\"\n",
        ")"
      ],
      "metadata": {
        "id": "GFsHbw4CN1A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = CustomTrainer(model_init=model_init, args=training_args,\n",
        "                  data_collator=data_collator,\n",
        "                  compute_metrics=compute_metrics,\n",
        "                  train_dataset=encoded_ds[\"train\"],\n",
        "                  eval_dataset=encoded_ds[\"valid\"],\n",
        "                  tokenizer=codebert_tokenizer)"
      ],
      "metadata": {
        "id": "_Gvs3SKwN0-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "SG1TrVTYNvdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# post-training analysis, testing, other logged code\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "83iS4klxODKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_ckpt_path = trainer.state.best_model_checkpoint\n",
        "print(best_ckpt_path)"
      ],
      "metadata": {
        "id": "vY0NBMQcObxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"PATH-TO-FOLDER\")"
      ],
      "metadata": {
        "id": "gnDNJ7Lo_5g0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}